---
title: "Lecture 9: Generating random variables -- transformations and wrap-up"
author: "Ciaran Evans"
output: beamer_presentation
---

## Recap: acceptance-rejection sampling

* Want to sample continuous r.v. $X \sim f$
* Can easily sample from a different density: $Y \sim g$, such that $\frac{f(t)}{g(t)} \leq c$


1. Sample $Y \sim g$
2. Sample $U \sim Uniform(0, 1)$
3. If $U \leq \dfrac{f(Y)}{cg(Y)}$, set $X = Y$. Otherwise, return to step 1.

**Question:** What are some potential downsides to the acceptance-rejection sampling method?

\vspace{3cm}


## Inefficiency in acceptance-rejection sampling

1. Sample $Y \sim g$
2. Sample $U \sim Uniform(0, 1)$
3. If $U \leq \dfrac{f(Y)}{cg(Y)}$, set $X = Y$. Otherwise, return to step 1.

$$P(\text{accept } Y | Y = y) = P \left( U \leq \frac{f(y)}{cg(y)}\right) = \ ??$$

$$P(\text{accept } Y) = \ ??$$

\vspace{4cm}

## Inefficiency in acceptance-rejection sampling

$Beta(2.7, 6.3)$ example from class activity:

```{r, echo=F, message=F, warning=F, fig.width = 4, fig.height=3.5, fig.align='left'}
gridpts <- seq(0, 1, 0.01)
x <- dbeta(gridpts, 2.7, 6.3)
y <- dunif(gridpts)
c = 2.68
plot(gridpts,x,xlim=c(0,1),ylim=c(0,2.8),type="l",col="blue",main="",xlab="",ylab="",
     lwd=1.5)
lines(gridpts,c*y,col="red", lwd=1.5)
#legend(.6,2,c("Target density, f(t)","c * g(t)"),lty=c(1,1),col=c("blue","red"))
```

Here $c = 2.7$. About many samples from $g$ would I need to get 1000 samples from $f$?

\vspace{2cm}

## Inefficiency in acceptance-rejection sampling

:::: {.columns}

::: {.column width="50%"}

```{r, echo=F, message=F, warning=F, fig.width = 4, fig.height=3.5, fig.align='left'}
gridpts <- seq(0, 1, 0.01)
x <- dbeta(gridpts, 2.7, 6.3)
y <- dunif(gridpts)
c = 2.68
plot(gridpts,x,xlim=c(0,1),ylim=c(0,2.8),type="l",col="blue",main="",xlab="",ylab="",
     lwd=1.5)
lines(gridpts,c*y,col="red", lwd=1.5)
#legend(.6,2,c("Target density, f(t)","c * g(t)"),lty=c(1,1),col=c("blue","red"))
```

:::

::: {.column width="50%"}

```{r, echo=F, message=F, warning=F, fig.width = 4, fig.height=3.5, fig.align='left'}
gridpts <- seq(0, 1, 0.01)
x <- dbeta(gridpts, 2.7, 6.3)
y <- dunif(gridpts)
c = 2.68
plot(gridpts,x,xlim=c(0,1),ylim=c(0,2.8),type="l",col="blue",main="",xlab="",ylab="",
     lwd=1.5)
lines(gridpts,1.5*dnorm(gridpts, mean=0.22, sd=0.22),col="red", lwd=1.5)
#legend(.6,2,c("Target density, f(t)","c * g(t)"),lty=c(1,1),col=c("blue","red"))
```

:::

::::

**Question:** Which of these two candidate densities $g$ would you prefer? 

\vspace{2cm}

## Drawbacks of acceptance-rejection sampling

1. Need to find a suitable candidate $g$
2. Requires more samples $Y \sim g$ than we get from target $f$ (because we reject some samples)
    * Want $g$ to be as close as possible to $f$, to accept as many samples as possible
3. Calculating $f(Y)$ for candidate draws $Y \sim g$ may be expensive for some distributions

**Project 1:** Modifying the acceptance-rejection method to address these drawbacks

**Today:** Another approach to generating random variables


```{r, include=F}
log_f_beta <- function(x, a, b){
  (a-1)*log(x) + (b-1)*log(1-x) - log(beta(a, b))
}

log_f_prime_beta <- function(x, a, b){
  (a-1)/x - (b-1)/(1-x)
}

get_intervals <- function(x, k, ls, dls, lower_endpoint, upper_endpoint){
  z <- c(lower_endpoint,
         (ls[2:k] - ls[1:(k-1)] - 
            x[2:k]*dls[2:k] + x[1:(k-1)]*dls[1:(k-1)])/(
              dls[1:(k-1)] - dls[2:k]
            ),
         upper_endpoint)
  
  p <- ifelse(abs(dls) < sqrt(.Machine$double.eps), # tolerance for "near equality"
              exp(ls - x*dls)*(z[2:(k+1)] - z[1:k]),
              exp(ls - x*dls)*(exp(z[2:(k+1)] * dls) - exp(z[1:k]*dls))/dls)
  
  output <- list("z" = z,
                 "p" = p)
  
  return(output)
}
```


```{r, include=F, echo=F, message=F, warning=F, fig.width = 4, fig.height=3.5, fig.align='left'}
k <- 5
x <- seq(0.05, 0.95, length = k)
log_f <- function(t){log_f_beta(t, 2.7, 6.3)}
log_f_prime <- function(t){log_f_prime_beta(t, 2.7, 6.3)}

intervals <- get_intervals(x = x, k = k, ls = log_f(x), dls = log_f_prime(x), 
                           lower_endpoint = 0, upper_endpoint = Inf)
z <- intervals$z

gk <- function(t, x, z){
  idx <- sapply(t, function(ti){min(which(ti < z)) - 1})
  exp(log_f(x[idx]) - x[idx]*log_f_prime(x[idx]) + t*log_f_prime(x[idx]))
}

plot(gridpts,dbeta(gridpts, 2.7, 6.3), xlim=c(0,1), ylim=c(0,3.5), type="l", 
     col="blue", main="", xlab="", ylab="", lwd=1.5)
lines(gridpts, gk(gridpts, x, z), lwd=1.5)
```


## Generating a Normal random variable

Suppose we want to simulate $X \sim N(0, 1)$

$$f_X(x) = \frac{1}{\sqrt{2 \pi}} \exp \left\lbrace -\frac{1}{2} x^2 \right\rbrace$$

$$F_X(t) = \ ?$$

## Box-Muller Transformation

## Box-Muller in practice

```{r}
# generate 1000 samples
u1 <- runif(1000)
u2 <- runif(1000)
x1 <- sqrt(-2*log(u1)) * cos(2*pi*u2)
x2 <- sqrt(-2*log(u1)) * sin(2*pi*u2)
```

**Question:** How can I check that the samples match the desired $N(0, 1)$ distribution?

## Box-Muller in practice

```{r}
# generate 1000 samples
u1 <- runif(1000)
u2 <- runif(1000)
x1 <- sqrt(-2*log(u1)) * cos(2*pi*u2)
x2 <- sqrt(-2*log(u1)) * sin(2*pi*u2)
```

```{r, echo=F, message=F, fig.width=6, fig.height=4}
gridpts <- seq(-3, 3, 0.01)
plot(gridpts, ecdf(x1)(gridpts), type="l", lwd=1.5, xlab="x", ylab="cdf")
lines(gridpts, pnorm(gridpts), col="orange", lwd=1.5)
legend(0.5,0.5,c("Empirical cdf X1","N(0, 1) cdf"),lty=c(1,1),col=c("black","orange"))
```

## Box-Muller in practice


```{r, echo=F, message=F, fig.width=3, fig.height=3}
library(tidyverse)
data.frame(x1, x2) |>
  ggplot(aes(x = x1, y = x2)) +
  geom_point(size=0.5) +
  theme_bw()
```


## Other Normals

Suppose that $Z \sim N(0, 1)$. How do I get $X \sim N(\mu, \sigma^2)$?

\vspace{4cm}

## A few other transformations

* If $X \sim N(\mu, \sigma^2)$, then $e^X \sim Lognormal(\mu, \sigma^2)$

* If $Z_1,...,Z_k \overset{iid}{\sim} N(0, 1)$, then 

$$\sum \limits_{i=1}^k Z_i^2 \sim \ ?$$

* If $V_1 \sim \chi^2_{d_1}$ and $V_2 \sim \chi^2_{d_2}$ are independent, then 

$$\frac{V_1/d_1}{V_2/d_2} \sim \ ?$$

* If $Y_1 \sim Gamma(\alpha, \theta)$ and $Y_2 \sim Gamma(\beta, \theta)$ are independent, then

$$\frac{Y_1}{Y_1 + Y_2} \sim ?$$

## Summary (so far)

Methods to generate random variables, in rough order of preference:

1. Use inverse transform method (if inverse cdf is tractable)
2. Find a different transformation (if possible)
3. Acceptance-rejection sampling (perhaps with modifications)

## Homework 3

[https://sta379-s25.github.io/homework/hw3.html](https://sta379-s25.github.io/homework/hw3.html)

* Practice generating random variables
* Accept and submit coding portion of assignment on GitHub Classroom
* Collaboration encouraged on homework, but everyone must submit their own work and acknowledge collaborators
    