---
title: "Challenge 6: Inverse variance weighting"
output: 
  rmdformats::robobook:
    css: "homework.css"
    highlight: pygments
link-citations: yes
---

**Due:** Friday April 25, 10pm

**Submission:** For this assignment, you will submit one file: a pdf of your written work to Canvas

**Grading:** You will receive a score out of 7 for the number of questions mastered. To master a question, you must satisfy the following requirements:

* Your written solutions must be complete and correct, with no missing steps or mistakes
* Responses to questions asking for description or explanation must be complete, thorough, and correct
* Your work must abide by the academic honesty requirements in the syllabus 

If you make an honest effort to answer all questions, but you do not master all of the questions on your first submission, you will have one resubmission attempt after receiving feedback.

# Inverse varaince weighting

Weighted least squares, and the iterative model estimation procedure used for GLMs (which can also be posed as an iterative weighted least squares procedure!) rely on *inverse variance weighting*. The purpose of this assignment is to justify that inverse variance weighting is an "optimal" choice.

To motivate inverse variance weighting, consider the follow scenario. Let $X_1,...,X_n$ be independent (but *not* necessarily identically distributed) real-valued random variables, and suppose that $\mathbb{E}[X_i] = \mu$ for all $i$ (the variables share a common mean). Let $\sigma_i^2 = Var(X_i)$, allowing the variances to be different for each $X_i$.

Now consider the weighted sum

$$Y = \sum \limits_{i=1}^n w_i X_i,$$

with weights $w_i$ such that $\sum_i w_i = 1$.

1. **Question:** Show that for any choice of weights, $\mathbb{E}[Y] = \mu$ (i.e., $Y$ is an unbiased estimator of $\mu$).

2. **Question:** What choice of weights $w_i$ gives $Y = \overline{X}$ (the sample mean)?

3. **Question:** Find $Var(Y)$ in terms of the weights $w_i$ and the variances $\sigma_i^2$.

Now our goal is to show that inverse variance weighting (in which $w_i \propto 1/\sigma_i^2$) is *optimal*, in the sense that it minimizes the variance $Var(Y)$. (Remember, we often like small variances in statistics, particularly for unbiased estimators! Small variances mean, for example, narrower confidence intervals, which means we are better able to estimate quantities of interest).

How do we find the weights that minimize the variance? The same way we minimize lots of other functions -- by taking derivatives! Here, though, we have to impose an additional constraint on the weights: $\sum_i w_i = 1$. To minimize the variance under this constraint, we will use the method of *Lagrange multipliers*. (Note: you may not be familiar with this technique, depending on your calculus background. That's ok! I will walk you through the relevant steps below).

The method of Lagrange multipliers gives us the following quantity:

$$\mathcal{L} = \sum \limits_{i=1}^n w_i^2 \sigma_i^2 - \lambda\left( \sum \limits_{i=1}^n w_i - 1 \right)$$

Our goal is to minimize $\mathcal{L}$; the first part corresponds to $Var(Y)$, and the second part corresponds to the constraint on the weights. The unknowns for which we are trying to optimize are $w_1,...,w_n$ and $\lambda$ (which we have introduced to handle the constraint on the weights).

As usual, we need to take derivatives and set them equal to 0. However, there is more than one unknown, so we need to take a *partial* derivative for each unknown. 


4. **Question:** Calculate the partial derivatives $\frac{\partial \mathcal{L}}{\partial w_i}$ for each weight $w_i$, set equal to 0, and solve for $w_i$. Your answer should involve $\lambda$ and $\sigma_i^2$.

5. **Question:** Using the fact that $\sum_i w_i = 1$, solve for $\lambda$.

6. **Question:** Conclude that the weights $w_i$ which minimize $Var(Y)$ are proportional to $1/\sigma_i^2$.

7. **Question:** Using the inverse variance weights calculated in questions 4 and 5, find $Var(Y)$ in terms of the $\sigma_i^2$. (That is, inverse variance weighting achieves the smallest possible variance for $Y$ -- what is that variance?)