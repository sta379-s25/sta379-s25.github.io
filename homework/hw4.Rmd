---
title: "Homework 4"
output: 
  rmdformats::robobook:
    css: "homework.css"
    highlight: pygments
link-citations: yes
---

# Compass search

In class, we saw compass search in two dimensions -- for example, trying to fit a simple linear regression model and estimate the coefficients $\beta_0$ and $\beta_1$. The algorithm can be generalized to arbitrarily many dimensions (e.g., for fitting a model with multiple explanatory variables).

### Compass search algorithm

Let ${\bf x} = (x_1,...,x_d)^T$ be a $d$-dimensional vector, and suppose we wish to minimize $f({\bf x})$, a real-valued function of the vector ${\bf x}$.

Let ${\bf e}_1,...,{\bf e}_d$ be the *standard basis vectors*:

$${\bf e}_1 = \begin{pmatrix} 1 \\ 0 \\ 0 \\ \vdots \\ 0 \end{pmatrix}, \ {\bf e}_2 = \begin{pmatrix} 0 \\ 1 \\ 0 \\ \vdots \\ 0 \end{pmatrix}, ..., \ {\bf e}_d = \begin{pmatrix} 0 \\ 0 \\ \vdots \\ 0 \\ 1 \end{pmatrix}$$

That is, each ${\bf e}_i$ is a $d$-dimensional vector with a 1 in the $i$th entry and 0s everywhere else.

Compass search works like this:

1. Specify an initial estimate ${\bf x}^{(0)}$ and an initial step size $\Delta_0$ 
2. For $i = 1,...,d$, 
    * compute the points ${\bf x}_{i, 1} = {\bf x}^{(0)} + \Delta_0 {\bf e}_i$ and ${\bf x}_{i, 2} = {\bf x}^{(0)} - \Delta_0 {\bf e}_i$
    * calculate $f({\bf x}_{i, 1})$ and $f({\bf x}_{i, 2})$
3. Let ${\bf x}_{\text{new}}$ be the new point among the ${\bf x}_{i,j}$ with the smallest value of $f({\bf x}_{i, j})$
4. Update:
    * If $f({\bf x}_{\text{new}}) < f({\bf x}^{(0)})$, then ${\bf x}^{(1)} = {\bf x}_{\text{new}}$ and $\Delta_1 = \Delta_0$
    * Otherwise ${\bf x}^{(1)} = {\bf x}^{(0)}$ and $\Delta_1 = \alpha\Delta_0$, where $\alpha < 1$ shrinks the step size
5. Repeat from step 2 until the step size $\Delta < \varepsilon$ for some specified small number $\varepsilon$, or until a maximum number of iterations has been reached

A common value of $\alpha$ here is $\alpha = 0.5$ (i.e., divide the step size by 2). The value of $\varepsilon$ should be something small, like $\varepsilon = 1 \times 10^{-6}$

### Implementation

Compass search is really a big iterative loop. And loops are often slow in R. So for this assignment, you will implement compass search in C++.

However, most of our work is in R, so we will likely be in a scenario in which we have data and a function in R that we would like to optimize, but we want the optimization to happen in C++. This means we need to be able to write a function in R and pass it to a C++ function as an argument.

Here is an example of calling an R function from C++:

```{r}
# a simple R function that we define
example_f <- function(x){
  return(sum(x))
}

# C++ function that calls the R function
Rcpp::cppFunction('double test_eval(Function f, arma::vec x){
  double res = Rcpp::as<double>(f(x));
  return res;
}', depends = "RcppArmadillo")

# test_eval function now has c++ source code, but takes an R function as an argument
test_eval(example_f, 1:3)
```

**Note:**

* We pass the R function using the `Function` object type from `Rcpp`
* We have to convert the R function output into a double in C++ (`Rcpp::as<double>(f(x))`)




:::{.question}
#### Question 1 (.cpp file)

Write a C++ function called `compass_searchC` that implements the basic compass search algorithm described above. Your function should satisfy the following requirements:

Inputs:

* `f`: a `Function` (from R) that we want to optimize
* `x`: an `arma::vec`, the initial point ${\bf x}^{(0)}$
* `step_size`: a `double`, the initial step size $\Delta_0$. Default: `step_size = 1`
* `step_change`: a `double`, the value $\alpha$ which we multiply to shrink the step size. Default: `step_change = 0.5`
* `eps`: a `double`, the termination criterion $\varepsilon$ for compass search algorithm. Default: $1 \times 10^{-6}$
* `max_iter`: an `int`, the maximum number of iterations allowed. Default: 1000

Output: a `List`, containing the following named elements:

* `x`: the final estimate of the point ${\bf x}$ which minimizes $f$
* `iterations`: an integer, the number of iterations taken by the compass search algorithm

Additional requirements: 

* You may not use any existing compass search functions to write your code

*Hint*: You can create a matrix of $d$-dimensional standard basis vectors (that is, the $d$-dimensional identity matrix) with

`arma::mat basis(d, d, arma::fill::eye);`

**Test cases:** See the unit tests in `hw4_tests.R`

:::


### Modified compass search algorithm

The compass search algorithm described above requires $2d$ function evaluations at every step. When the dimensionality of our problem is large, that can be a lot of evaluations!

There are many ways we could consider modifying this compass search procedure. One option is to be a bit more efficient (hopefully) in the order in which we evaluate new points. Consider the following two adjustments:

* Instead of looking over all $2d$ possible new points ${\bf x}_{i,j}$ and choosing the point among them which minimizes $f$, we could move as soon as we find a point which decreases $f$
* Rather than looking over the $2d$ possible new points in arbitrary order, we can keep track of the direction of most recent movement, and search there first. If we need to move a lot in one direction, reordering the directions will be more efficient

The modified algorithm is then:

1. Specify an initial estimate ${\bf x}^{(0)}$ and an initial step size $\Delta_0$. Let ${\bf D} = \{ {\bf e}_1, {\bf e}_2,...,{\bf e}_d, -{\bf e}_1,...,-{\bf e}_d \}$ denote our initial ordering of the $2d$ test point directions
2. For each direction ${\bf v}$ in ${\bf D}$:
    * ${\bf x}_{\bf v} = {\bf x}^{(0)} + \Delta_0 {\bf v}$
    * calculate $f({\bf x}_{\bf v})$ 
    * If $f({\bf x}_{\bf v}) < f({\bf x}^{(0)})$: 
        * stop trying directions
        * ${\bf x}^{(1)} = {\bf x}_{\bf v}$
        * $\Delta_1 = \Delta_0$
        * Move direction ${\bf v}$ to the front of ${\bf D}$
3. If none of the test points ${\bf x}_{{\bf v}}$ were better than ${\bf x}^{(0)}$, set ${\bf x}^{(1)} = {\bf x}^{(0)}$ and $\Delta_1 = \alpha\Delta_0$, where $\alpha < 1$ shrinks the step size
4. Repeat from step 2 until the step size $\Delta < \varepsilon$ for some specified small number $\varepsilon$, or until a maximum number of iterations has been reached


### Reordering the direction

For example, suppose that $f$ is a function of 2 parameters (i.e., $d = 2$). Then, my initial search directions are

$${\bf D}^{(0)} = \{ {\bf e}_1, {\bf e}_2, -{\bf e}_1, -{\bf e}_2 \}$$

Suppose on the first iteration, ${\bf e}_1$ does not improve $f$, but ${\bf e}_2$ does. Then, the updated ordering of the directions should be

$${\bf D}^{(1)} = \{{\bf e}_2, {\bf e}_1, -{\bf e}_1, -{\bf e}_2\}$$

And now suppose on the second iteration, ${\bf e}_2$ and ${\bf e}_1$ do not improve $f$, but $-{\bf e}_1$ does. The updated ordering should then be

$${\bf D}^{(2)} = \{-{\bf e}_1, {\bf e}_2, {\bf e}_1, -{\bf e}_2\}$$


### Implementation

To implement this modified compass search algorithm, you can adapt your code from question 1. An important tweak is that we now need to keep track of the order in which we consider the different search directions, and update the order as we go.

One nice way of doing this could be to make a vector of indices for the $2d$ different search directions, and modify the order of that vector as we go. For this reordering, Armadillo vectors might not be the right choice of object for these indices. Rather, I suggest you use a `std::vector` object. This is a vector from the standard template library (STL) in C++, which provides a lot of useful data structures and functions for implementing common general-purpose algorithms. In particular, `std::rotate` allows you to move around the positions of vector elements.

Here is example C++ code creating the vector of indices $0,...,2d-1$ (remember C++ starts at 0):

```{Rcpp, eval = F}
std::vector<int> idx(2*d);
std::iota(idx.begin(), idx.end(), 0);
```

And here is an example of shifting the element at index $i$ to the front of the vector:

```{Rcpp, eval=F}
std::rotate(idx.begin(),idx.begin()+i, idx.begin()+i+1);
```

**Important:** `std::iota` and `std::rotate` modify the vector `idx` directly; you don't have to reassign the results to `idx`.


:::{.question}
#### Question 2 (.cpp file)

Write a C++ function called `mod_compass_searchC` that implements the modified compass search algorithm described above. Your function should satisfy the following requirements:

Inputs:

* `f`: a `Function` (from R) that we want to optimize
* `x`: an `arma::vec`, the initial point ${\bf x}^{(0)}$
* `step_size`: a `double`, the initial step size $\Delta_0$. Default: `step_size = 1`
* `step_change`: a `double`, the value $\alpha$ which we multiply to shrink the step size. Default: `step_change = 0.5`
* `eps`: a `double`, the termination criterion $\varepsilon$ for compass search algorithm. Default: $1 \times 10^{-6}$
* `max_iter`: an `int`, the maximum number of iterations allowed. Default: 1000

Output: a `List`, containing the following named elements:

* `x`: the final estimate of the point ${\bf x}$ which minimizes $f$
* `iterations`: an integer, the number of iterations taken by the compass search algorithm

Additional requirements: 

* You may not use anyone else's compass search functions to write your code
* At each iteration, stop searching once you have found a direction of improvement
* At each iteration, reorder the vector to put the new direction of improvement first

**Test cases:** See the unit tests in `hw4_tests.R`

:::


### Benefits of the modified algorithm

In the practice questions from February 7, you used compass search to fit a logistic regression model to medical school admissions data. 

Let $p_i = P(\text{Acceptance}_i = 1)$ be the probability of acceptance for the $i$th individual in the data. Then, the logistic regression model is

$$\log \left(\frac{p_i}{1 - p_i} \right) = \beta_0 + \beta_1 \text{GPA}_i$$
The function we are trying to minimize is

$$f(\beta_0, \beta_1) = - \sum \limits_{i=1}^n \left \lbrace \text{Acceptance}_i(\beta_0 + \beta_1 \text{GPA}_i) - \log(1 + e^{\beta_0 + \beta_1 \text{GPA}_i}) \right\rbrace$$

When we started at $(0, 0)$, with an initial step size of 1, compass search took over 11000 iterations to finish. If we plot the guesses $(\beta_0^{(k)}, \beta_1^{(k)})$ over time, we can see that compass search moves in a nice diagonal line from (0, 0) up to about (-19, 5.5):

```{r comp, include=F, cache=T}
library(Stat2Data)
data("MedGPA")

compass_search <- function(f, x, step_size = 1, trace=F){
  
  step_size <- 1
  cur_point <- x
  d <- length(x)
  compass_grid <- rbind(rep(0, d),
                        diag(d),
                        -1*diag(d))
  
  iter_count <- 0
  
  if(trace){
    track_pts <- c(cur_point)
  }
  
  while(step_size > 1e-6){
    test_points <- sweep(step_size*compass_grid, 2, cur_point, "+")
    f_vals <- apply(test_points, 1, f)
    
    idx <- which.min(f_vals)
    if(idx == 1){
      step_size <- step_size* 0.5
    } else {
      cur_point <- test_points[idx,]
    }
    
    iter_count <- iter_count + 1
    if(trace){
      track_pts <- rbind(track_pts, cur_point)
    }
    
  }
  if(trace){
    list("x"=cur_point, "iterations"=iter_count, "track_pts" = track_pts)
  } else {
    list("x"=cur_point, "iterations"=iter_count)
  }
  
}

logistic_loss <- function(beta){
  odds <- exp(beta[1] + beta[2]*MedGPA$GPA)
  p <- odds/(1 + odds)
  -1*sum(dbinom(MedGPA$Acceptance, 1, p, log=T))
}

res <- compass_search(logistic_loss, c(0, 0), trace=T)
```

```{r, echo=F}
plot(res$track_pts[,1], res$track_pts[,2], type="l",
     xlab = "Beta0 current guess", ylab = "Beta1 current guess",
     main = "Compass search path")
```

This means that compass search is really only moving in two directions here -- up and left. If we reorder the directions in the modified search algorithm, we should only need to look in two directions at most (we can ignore the "down" and "right" directions), which should speed up the search by something like a factor of 2.

Here is the code from February 7 to load the med school acceptance data and fit the logistic regression model in R:

```{r, message=F, warning=F}
library(Stat2Data)
data("MedGPA")

glm(Acceptance ~ GPA, family = binomial, data = MedGPA)
```

And here is the function $f(\beta_0, \beta_1)$ to minimize:

```{r}
logistic_loss <- function(beta){
  odds <- exp(beta[1] + beta[2]*MedGPA$GPA)
  p <- odds/(1 + odds)
  -1*sum(dbinom(MedGPA$Acceptance, 1, p, log=T))
}
```


:::{.question}
#### Question 3 (.html file)

Use compass search to fit the logistic regression model, starting from $(0, 0)$. Try both of your implementations. Do the end results agree? How many steps were required?

:::

:::{.question}
#### Question 4 (.html file)

Use the `bench::mark` function to compare the speed of your two compass search implementations on the logistic regression model for the medical school acceptance data. How does the speed compare?

:::

:::{.question}
#### Question 5 (.html file)

Now let's consider more explanatory variables: 

$$\log \left(\frac{p_i}{1 - p_i} \right) = \beta_0 + \beta_1 \text{GPA}_i + \beta_2 \text{MCAT}_i$$

Use the `bench::mark` function to compare the speed of your two compass search implementations on this logistic regression model, beginning at the point $(0, 0, 0)$. How does the speed compare?

**Note:** You will need to modify the `logistic_loss` function!

:::


# Weighted least squares

One of the assumptions we commonly make in a standard linear regression model is *constant variance*. That is, the variance of the residuals is the same for all values of our explanatory variable(s). 

Here is an example of data for which that assumption might not be satisfied. Each point represents a county in the US, and we are plotting the number of doctors (MDs) in that county vs. the number of hospitals in the county. We can see that the variability in the number of doctors increases with the number of hospitals.

```{r, message=F, warning=F, echo=F}
library(Stat2Data)
library(tidyverse)
data("CountyHealth")

CountyHealth |>
  ggplot(aes(x = Hospitals, y = MDs)) +
  geom_point() +
  theme_bw()
```

One way to address non-constant variance is to transform our response variable. However, transforming the response changes the interpretation of our model, which is often not what we want.

Another option is **weighted least squares**.

### Weighted least squares

In the usual linear regression model with $p$ explanatory variables, we write

$$Y_i = \beta_0 + \beta_1 X_{i,1} + \beta_2 X_{i,2} + \cdots + \beta_p X_{i,p} + \varepsilon_i$$

In matrix form, this becomes

$${\bf y} = \begin{bmatrix} Y_1 \\ Y_2 \\ \vdots \\ Y_n \end{bmatrix} = \begin{bmatrix} 1 & X_{1,1} & \cdots & X_{1,p} \\ 1 & X_{2,1} & \cdots & X_{2,p} \\ \vdots & \vdots & \cdots & \vdots \\ 1 & X_{n,1} & \cdots & X_{n,p} \end{bmatrix} \begin{bmatrix} \beta_0 \\ \beta_1 \\ \vdots \\ \beta_p \end{bmatrix} =  {\bf X}_D \beta$$

Least-squares linear regression estimates the coefficients $\beta$ by minimizing

$$L(\beta) = \sum \limits_{i=1}^n (Y_i - \beta_0 - \beta_1 X_{i,1} - \cdots - \beta_p X_{i,p})^2$$

**Weighted least squares** estimates the coefficients $\beta$ by minimizing 

$$L_W( \beta) = \sum \limits_{i=1}^n w_i(Y_i - \beta_0 - \beta_1 X_{i,1} - \cdots - \beta_p X_{i,p})^2$$

where $w_i > 0$ is the *weight* that we place on the $i$th observation when fitting the model. If we are worried about non-constant variance, a common choice is $w_i = 1/Var(\varepsilon_i)$; this is called *inverse variance weighting*.

### Solving for $\beta$

:::{.question}
#### Question 6 (written)

Show that for the weighted least squares problem above,

$$\nabla L_W(\beta)  = -2 {\bf X}_D^T {\bf W} ({\bf y} - {\bf X}_D \beta)$$

where ${\bf W} = \text{diag}(w_1,...,w_n)$ is a diagonal matrix with the weights $w_i$ on the main diagonal. Show all work.

:::

:::{.question}
#### Question 7 (written)

Using your answer to question 6, show that the estimated coefficients for weighted least squares regression are given by

$$\widehat{\beta} = ({\bf X}_D^T {\bf W} {\bf X}_D)^{-1} {\bf X}_D^T {\bf W} {\bf y}$$

Show all work.

:::




:::{.question}
#### Question 8 (.R file)

Write an R function to compute $\widehat{\beta}$ for the weighted least squares method. Your function should satisfy the following requirements:

Inputs:

* `X`: the design matrix ${\bf X}_D$
* `y`: the vector of responses ${\bf y}$
* `w`: a vector of weights $w_1,...,w_n$ (which will be used to construct the matrix ${\bf W}$)

Outputs: the vector of estimated coefficients $\widehat{\beta} = ({\bf X}_D^T {\bf W} {\bf X}_D)^{-1} {\bf X}_D^T {\bf W} {\bf y}$

Error handling:

* Throw an error if all weights are 0
* Throw an error if any weights are negative

Additional requirements: 

* You may not use any existing linear model implementations (e.g., the `lm` function)

**Test cases:** See the unit tests in `hw4_tests.R`

:::


### Why do we care?

Why does it matter whether we use weighted least squares to account for non-constant variance? It turns out that, if there is non-constant variance, the variance of the weighted least squares estimate is smaller -- and in statistics, we like smaller variances!

In this part of the assignment, we will demonstrate that with a simulation. The following code simulates 100 observations $(X_i, Y_i)$ from the model

$$Y_i = \beta_0 + \beta_1 X_i + \varepsilon_i$$

where $\beta_0 = 0, \ \beta_1 = 1, \ X_i \sim Uniform(0, 10)$, and $Var(\varepsilon_i) = X_i^2$ (non-constant variance!). We then calculate the estimated slope $\widehat{\beta}_1$ for the ordinary least squares fit and the weighted least squares fit.


```{r}
# sample x
x <- runif(100, 0, 10)

# create y
y <- 0 + 1*x + rnorm(100, 0, sd=x)

# get the slope estimate with ordinary least squares
coef(lm(y ~ x))[2]

# get the slope estimate with weighted least squares
coef(lm(y ~ x, weights = 1/x^2))[2]
```
:::{.question}
#### Question 9 (.html file)

Using the code provided, repeat the simulation $1000$ times, calculating ordinary least squares and weighted least squares estimates of $\beta_1$ for each simulation repetition. Report the mean and variance of $\widehat{\beta}_1$ for each method. Which method gives better estimates of $\beta_1$ when the data have non-constant variance?

:::



# Robust regression

In this section of the assignment, you will apply gradient descent to fit another type of regression model, called a *robust* regression model.


### Motivation: the impact of outliers

To motivate this model, we will examine a dataset on characteristics of 45 different US occupations in 1950. Each row represents an occupation, and the following variables are available:

* `prestige`: the fraction of respondents in a social survey who rated the occupation as "good" or better in prestige
* `income`: Percentage of occupational incumbents in the 1950 US Census who earned \$3,500 or more per year (about \$36,000 in 2017 US dollars)
* `education`: Percentage of occupational incumbents in 1950 who were high school graduates

Suppose we want to model prestige as a function of income and education. We could fit a linear regression model and examine the diagnostics. Here is the plot of Cook's distance (which, you will recall, is used to check for influential points that may be affecting the fit of the model):

```{r, message=F, warning=F}
library(car)

ols_mod <- lm(prestige ~ income + education, data=Duncan)
plot(ols_mod, which=4)
```

From this plot, we can see that two occupations -- minister and train conductor -- are potentially influential. (This is because both occupations have a relatively high prestige, but low income and higher education).

What happens when we remove these two points? Here are the coefficients with the full data:

```{r}
coef(ols_mod)
```

And here are the coefficients without the two potential outliers:

```{r}
ols_mod_2 <- lm(prestige ~ income + education, data=Duncan[-c(6, 16),])
coef(ols_mod_2)
```

This is a pretty noticeable change!


### What to do about outliers?

If we wish to use ordinary least-squares to fit our model, we could report the model (and any conclusions) with and without the influential points. Another option, however, is to use a different method to estimate the coefficients -- ideally one which is less susceptible to outliers.

Why is least-squares linear regression susceptible to outliers? Because we are using the *squared* residuals, $(Y_i - \widehat{Y}_i)^2$. Influential points have big residuals, which get much bigger when we square them!

A natural fix is then to use a *different* loss function to fit our model.

### Huber loss

The Huber loss function $\rho$ is given by

$$\rho(y - \widehat{y}) = \begin{cases} \frac{1}{2} (y - \widehat{y})^2 & |y - \widehat{y}| \leq \gamma \\ \gamma |y - \widehat{y}| - \frac{1}{2}\gamma^2 & \text{else} \end{cases}$$

That is, when the residual $y - \widehat{y}$ is small, we're comfortable using the usual squared error $(y - \widehat{y})^2$ from OLS. When the residual is large, however, we instead use the *absolute* error $|y - \widehat{y}|$, which places less weight on "big" residuals than the squared error.

The parameter $\gamma$ tells us when to transition from squared errors to absolute errors. Here is a plot of the Huber loss function with $\gamma = 1$. At -1 and 1, the function transitions from quadratic to linear:

```{r, echo=F, message=F, warning=F}
huber <- function(u, gamma){
  ifelse(abs(u) <= gamma, 0.5*u^2, gamma*abs(u) - 0.5*gamma^2)
}

x <- seq(-2, 2, 0.01)
plot(x, huber(x, 1), type="l", ylab = "Huber loss", title = "Huber loss with gamma = 1")
abline(v = c(-1, 1), lty=2)
```

### Robust regression

The robust regression model estimates the coefficients $\beta$ by optimizing a robust loss like the Huber loss function, rather than the sum of squared residuals. For example, suppose we have the model

$$Y_i = \beta_0 + \beta_1 X_{i,1} + \beta_2 X_{i,2} + \cdots + \beta_p X_{i,p} + \varepsilon_i$$

Then, robust regression minimizes

$$L(\beta) = \sum \limits_{i=1}^n \rho(Y_i - \beta_0 - \beta_1 X_{i,1} - \beta_2 X_{i,2} - \cdots - \beta_p X_{i,p})$$

In R, we can fit this with the `rlm` function from the `MASS` package:

```{r, message=F, warning=F}
library(MASS)
robust_mod <- rlm(prestige ~ income + education, data=Duncan)
coef(robust_mod)
```

We can see that the coefficients on `income` and `education` lie somewhere between the OLS coefficients on the full data, and the OLS coefficients *without* the influential points.

### Fitting robust regression models

Unlike OLS, there is not a closed-form solution for the coefficients estimated by robust regression. Instead, we need to use an iterative procedure, and there are several possibilities. One option is a derivative-free method like Nelder-Mead. If you use Nelder-Mead from the `optim` function in R, starting at the coefficients estimated by OLS with the full data, and using $\gamma = 1$ for the Huber loss, you should get estimates

```{r, echo=F}
# huber loss function
huber_loss <- function(u, gamma = 1){
  ifelse(abs(u) <= gamma, 0.5*u^2, gamma*abs(u) - 0.5*gamma^2)
}

# huber influence function
huber_psi <- function(u, gamma = 1){
  ifelse(abs(u) <= gamma, u, gamma * sign(u))
}

# robust regression with huber loss function

f <- function(X, y, beta, gamma=1){
  sum(huber_loss(y - c(X %*% beta), gamma))
}

f_grad <- function(X, y, beta, gamma=1){
  -1*t(X) %*% huber_psi(y - c(X %*% beta), gamma)
}


### fitting the model with Nelder-Mead
f_nm <- function(beta){
  sum(huber_loss(y - c(X %*% beta), gamma = 1))
}

X <- cbind(1, Duncan$income, Duncan$education)
y <- Duncan$prestige
cur_beta <- lm(prestige ~ income + education, data=Duncan) |> coef()
optim(cur_beta, f_nm, method = "Nelder-Mead")$par
```

**Note:** This is different from the `rlm` output because `rlm` is using a different value of $\gamma$ in the Huber loss. We will think about selecting a good value of $\gamma$ in the next assignment.


On this assignment you will explore gradient descent to fit robust regression models; on a future assignment, you will see another option that leverages the weighted least squares solutions you found above, called *Iteratively Reweighted Least Squares* (IRLS).


:::{.question}
#### Question 10 (written)

Find an expression for the gradient $\nabla L(\beta)$ for robust regression with the Huber loss function.

**Hint:** Huber loss is piecewise, so your gradient will be a sum of piecewise functions!

:::

Now let's try gradient descent!

:::{.question}
#### Question 11 (.html file)

Using the gradient you found in the previous question, try gradient descent with a step size of $\alpha = 0.01$. For the Huber loss, use $\gamma = 1$ for now. As your starting point $\beta^{(0)}$, use the coefficient estimates from ordinary least squares with the full data:

```{r}
coef(ols_mod)
```

Perform 10 gradient descent steps, and track your updates $\beta^{(k)}$ for the 10 iterations. Plot the updates; what do you notice is happening?

:::

What you should observe is that gradient descent bounces between two values. Rather than getting closer to the true solution, it just alternates! This is one potential failure of gradient descent when the step size $\alpha$ is poor.

Why does this happen? It has to do with the gradient.

:::{.question}
#### Question 12 (.html file)

Calculate the gradient $\nabla L$ at the two points between which gradient descent is alternating when $\alpha = 0.01$. How are these two gradients related? Explain why this results in gradient descent alternating between the two points.

:::

Clearly, $\alpha = 0.01$ was not the right step size. Let's try a smaller step size:


:::{.question}
#### Question 13 (.html file)

Try gradient descent for the robust regression with a step size of $\alpha = 0.00001$. As before, use $\gamma = 1$ for the Huber loss. As your starting point $\beta^{(0)}$, use the coefficient estimates from ordinary least squares with the full data:

```{r}
coef(ols_mod)
```

Perform 10 gradient descent steps, and track your updates $\beta^{(k)}$ for the 10 iterations. Plot the updates; what do you notice is happening?

:::

With a smaller step size, gradient descent no longer alternates between two fixed points. However, it hasn't made much progress after 10 iterations! Let's try more iterations:


:::{.question}
#### Question 14 (.html file)

Try gradient descent for the robust regression with a step size of $\alpha = 0.00001$. As before, use $\gamma = 1$ for the Huber loss. As your starting point $\beta^{(0)}$, use the coefficient estimates from ordinary least squares with the full data:

```{r}
coef(ols_mod)
```

Perform 500000 (yes, 500000) gradient descent steps, and track your updates $\beta^{(k)}$ for the iterations. Plot the updates; at what point does it seem like gradient descent is converging to the true minimum? 

:::


### Why is gradient descent so slow??

First, it is important to note that gradient descent isn't always slow; done well, it is a good method for optimizing functions. However, gradient descent with a single fixed step size $\alpha$ does poorly when the rate of change in our function is different for the different coordinates.

What do I mean by that? We have been starting at the OLS estimates for the coefficients (a very reasonable place to begin our iterative procedure, since it should be in the right ballpark of the robust estimates). The gradient of our robust loss function at that starting place is

```{r, echo=F}
huber_psi <- function(u, gamma = 1){
  ifelse(abs(u) <= gamma, u, gamma * sign(u))
}

huber_grad <- function(X, y, beta){
  -1*t(X) %*% huber_psi(y - c(X %*% beta))
}

cur_beta <- lm(prestige ~ income + education, data=Duncan) |> coef()
X <- cbind(1, Duncan$income, Duncan$education)
huber_grad(X, Duncan$prestige, cur_beta)
```

Notice how the magnitude of the first coordinate is *much* smaller than the magnitude of the second and third coordinates. This means that at our initial $\beta^{(0)}$ (the OLS estimates), the function $L(\beta)$ is much more sensitive to changes in $\beta_1$ and $\beta_2$ than it is to changes in $\beta_0$. 

When we choose the step size, we are hampered by the magnitude of the coordinates of the gradient. If we pick a step size too big, we'll move a good distance in the $\beta_0$ coordinate, but *much* too far in the $\beta_1$ and $\beta_2$ coordinates. If we pick a smaller step size, we're moving the right amount for $\beta_1$ and $\beta_2$, but hardly moving at all for $\beta_0$. In your plot of the gradient descent updates from the 500000 iterations, you should see that we got good estimates of $\beta_1$ and $\beta_2$ much faster than we got good estimates of $\beta_0$.

### What can we do??

There are several approaches we could take to modify the gradient descent algorithm. One of them is to use a different step size for each coordinate. Another is to use information about the curvature of the function $L(\beta)$ -- that is, the rate of change in the gradient, which is the *second* derivative. We will talk about both of these approaches in class.

Here's another option, which is simple to implement, often helps, and is a common pre-processing step in many machine learning models: **scale the explanatory variables**. By scaling, I mean subtract the mean and divide by the standard deviation.

Scaling the explanatory variables will change our estimated coefficients, but it won't change our predictions. Here is an example for the OLS fit:

```{r}
ols_mod <- lm(prestige ~ income + education, data=Duncan)
ols_mod_scaled <- lm(prestige ~ scale(income) + scale(education), data=Duncan)

coef(ols_mod)
coef(ols_mod_scaled)

all.equal(ols_mod$fitted.values, ols_mod_scaled$fitted.values)
```

Our coefficients are still interpretable after scaling, too. For example, $\widehat{\beta}_1 = 14.63$ would now be interpreted as the expected changed in prestige associated with a one-standard deviation increase in income, holding education fixed. 

Now, let's look at the gradient of the loss function when starting at the OLS solution for the *scaled* variables:

```{r, echo=F}
# huber loss function
huber_loss <- function(u, gamma = 1){
  ifelse(abs(u) <= gamma, 0.5*u^2, gamma*abs(u) - 0.5*gamma^2)
}

# huber influence function
huber_psi <- function(u, gamma = 1){
  ifelse(abs(u) <= gamma, u, gamma * sign(u))
}

# robust regression with huber loss function

f <- function(X, y, beta, gamma=1){
  sum(huber_loss(y - c(X %*% beta), gamma))
}

f_grad <- function(X, y, beta, gamma=1){
  -1*t(X) %*% huber_psi(y - c(X %*% beta), gamma)
}

# fixing the issue
income_scaled <- scale(Duncan$income)
education_scaled <- scale(Duncan$education)
X_scaled <- cbind(1, income_scaled, education_scaled)

cur_beta <- lm(Duncan$prestige ~ income_scaled + education_scaled) |> coef()
#cur_beta
f_grad(X_scaled, Duncan$prestige, cur_beta)
```

The magnitudes of the different coordinates are much more similar!


:::{.question}
#### Question 15 (.html file)

Using the scaled explanatory variables, use gradient descent for the robust regression. Use a step size $\alpha = 0.1$, and use $\gamma = 1$ for the Huber loss. As your starting point $\beta^{(0)}$, use the coefficient estimates from ordinary least squares with the full data and scaled explantory variables:

```{r}
coef(ols_mod_scaled)
```

Perform 30 iterations. Plot the updates; at what point does it seem like gradient descent is converging to the true minimum? 

:::




