---
title: "Homework 3"
output: 
  rmdformats::robobook:
    css: "homework.css"
    highlight: pygments
link-citations: yes
---

**Due:** Friday February 7, 10:00am

**Submission:** For this assignment, you will submit two files:

* An R script (`hw3_code.R`) containing your functions for questions 1, 3, and 6
* A pdf containing your written answers to questions 2, 4, and 5

**Mastery:** To master this assignment, you must satisfy the following requirements:

* Your code must pass all specified unit tests, *as written* (you may not modify the unit tests which I provide)
* Your code must meet all requirements described in the questions below (matching the requested inputs and outputs)
* You must comment your code
* Your written solutions must be complete and correct, with no missing steps or mistakes
* Your work must abide by the academic honesty requirements in the syllabus 

If you make an honest effort to answer all questions, but you do not master all of the questions on your first submission, you will have one resubmission attempt after receiving feedback.

**Resources:** 

* *Statistical Computing with R* (Rizzo), Chapter 3
* *Computational Statistics* (Givens and Hoeting), Chapter 6.2

Both books are available electronically through the ZSR library

## Preliminaries: assessing generated random variables

In class, we have talked about several different methods for generating random variables. A crucial step when simulating a random variable is checking that it does actually follow the desired distribution!

One way we did this in class was to use the empirical cdf, and compare the empirical cdf to the true cdf (which should be known if we are trying to simulate from a specific distribution). While visually comparing cdfs is a valuable approach, we might wonder whether we can formalize this comparison. That is, can we conduct a hypothesis test that would detect significant differences between the empirical cdf and the true cdf?

The answer is yes, and comes in the form of the **Kolmogorov-Smirnov** (KS) test. In the first part of this assignment you will implement the KS test, to get a better feel for what the test is doing. In the remainder of the assignment, the KS test will be used in our unit tests to help assess the random variables we generate.

### The Kolmogorov-Smirnov test

Suppose we observe a univariate random sample $X_1,...,X_n$ of size $n$, from some population. We are interested in testing whether the generating distribution for this sample is some specified distribution; for example, we may wish to know whether the data come from a $Uniform(0, 1)$ distribution, or a $N(0, 1)$ distribution.

Formally, let $F$ be the (unknown) distribution function (cdf) for the sample, i.e. $X_1,...,X_n \overset{iid}{\sim} F$. Let $F_0$ be the *hypothesized* distribution function (e.g., a particular uniform, or standard normal, or something else). We wish to test the following hypotheses:

$$H_0: F = F_0$$
$$H_A: F \neq F_0$$

Note that this is a very strong null hypothesis. Often when testing hypotheses, we ask something about a single parameter, like the mean. Here, however, we are asking whether we know the entire *distribution*. In certain cases, however, this is the right kind of null hypothesis; when generating random numbers, for example, we really do want them to be $Uniform(0, 1)$.

How does one test such hypotheses? One option, called the *Kolmogorov-Smirnov* (KS) test, utilizes our old friend the empirical cdf. In particular, the KS test is built by examining the maximal distance between the empirical cdf $\widehat{F}$, and the hypothesized cdf $F_0$:

$$D = \sup \limits_t |\widehat{F}(t) - F_0(t)|$$

Now, you may be wondering, how does one compute a supremum like this, if it is over uncountably many points? Fortunately, properties of the cdf mean that we really only need to consider the $n$ observed points. 

Let $X_{(1)},...,X_{(n)}$ denote the *ordered* sample (from smallest to largest). Then, it turns out that 

$$D = \max \limits_{1 \leq i \leq n} \left \lbrace F_0(X_{(i)}) - \frac{i-1}{n}, \frac{i}{n} - F_0(X_{(i)})\right\rbrace$$

We then reject the null hypothesis that $F = F_0$ when our empirical cdf $\widehat{F}$ (i.e., our best estimate of the true cdf $F$) is far from the hypothesized cdf $F_0$. That is, we reject when $D$ is large!

Here is an example of the KS test in action, using the `ks.test` function in R. First, we generate 100 samples from a $Uniform(0, 1)$ distribution, then test if they come from a $Uniform(0, 1)$:

```{r}
set.seed(27)
x <- runif(100)
ks.test(x, punif)
```
Here the test statistic is $D = 0.084932$, and the p-value is 0.4664. That's quite a high p-value, so we fail to reject. Since the data really did come from a $Uniform(0, 1)$ distribution, that's good news!

On the other hand, if we sample from a standard normal, but hypothesize that our data comes from a uniform distribution, we would hope to *reject* the null:

```{r}
set.seed(27)
x <- rnorm(100)
ks.test(x, punif)
```

### Computing the p-value

The `ks.test` function in R provides a p-value, but it is not immediately clear where that p-value comes from. What distribution should this maximal difference $D$ follow?

The math is a bit beyond the scope of this course, but it turns out that asymptotically (that is, for a sufficiently large sample size),

$$P(\sqrt{n} D \leq t) \approx \frac{\sqrt{2 \pi}}{t} \sum \limits_{k=1}^\infty e^{-(2k - 1)^2 \pi^2 / (8t^2)}$$

Now, an infinite sum here seems tricky to calculate, but fortunately it converges quickly; you probably don't need more than about 30 terms to get a good approximation of the cdf, so 

$$P(\sqrt{n} D \leq t) \approx \frac{\sqrt{2 \pi}}{t} \sum \limits_{k=1}^{30} e^{-(2k - 1)^2 \pi^2 / (8t^2)}$$

### Summary

To summarize, the KS test works like this: given a sample $X_1,...,X_n$, the test statistic is

$$D = \max \limits_{1 \leq i \leq n} \left \lbrace F_0(X_{(i)}) - \frac{i-1}{n}, \frac{i}{n} - F_0(X_{(i)})\right\rbrace$$

and the approximate p-value is

$$\text{p-value} \approx 1 - \frac{\sqrt{2 \pi}}{\sqrt{n} D} \sum \limits_{k=1}^{30} e^{-(2k - 1)^2 \pi^2 / (8 n D^2)}$$

:::{.question}
#### Question 1

Write a function in R called `my_kstest`, which satisfies the following requirements:

Inputs:

* `x`: a vector, containing the random sample
* `cdf`: a function (such as `pnorm`, `punif`, etc.) specifying the distribution function $F_0$ under the null hypothesis

Output: a list, containing the following named elements:

* `D`: the test statistic $D$
* `pval`: the p-value for the test

Additional requirements:

* You may not use any existing KS test implementations to write your function
* You must implement the p-value approximation using the cdf defined above; you may not use any R packages that provide functions for the Kolmogorov distribution

**Test cases:** See the unit tests in `hw3_tests.R`

:::


## The inverse transform method

In previous activities and assignments, you have worked on generating (pseudo-random) $Uniform(0, 1)$ samples. One method of generating other types of random variables is to use these uniform samples as a building block, and applying transformations to produce the distribution we want. When tractable, an attractive approach is the *inverse transform method*, in which we apply the inverse cdf to a uniform sample.

**Definition:** (inverse transform method) Let $U \sim Uniform(0, 1)$ be a uniform random variable, and let $F$ be the cdf of a distribution we would like to sample from (either continuous or discrete). Define
$$F^{-1}(u) = \inf \{x: F(x) \geq u\}$$
and let $X = F^{-1}(U)$. Then, $P(X \leq t) = F(t)$, i.e. $X$ has the desired cdf and is therefore a sample from the desired distribution.

In the following questions, you will use the inverse transform method to simulate from a specified distribution. You will submit one written question in which you derive the inverse cdf, and one code-based question in which you use the inverse cdf method to sample from the desired distribution.


### Laplace distribution


The $Laplace(\mu, b)$ distribution has pdf

$$f(x) = \frac{1}{2b} \exp\left\lbrace - \frac{|x - \mu|}{b} \right\rbrace$$
where $x, \mu \in \mathbb{R}$ and $b > 0$.


:::{.question}
#### Question 2 (written)

Derive the inverse transform for the $Laplace(\mu, b)$ distribution. That is, if $U \sim Uniform(0, 1)$, find $F^{-1}(U)$ as a function of $U$. Show all work.

:::


Now let's implement a function to simulate from the Laplace distribution. Before we do, however, we should think about possible errors that could arise when calling our function. Note from the definition above that the parameter $b$ for the Laplace must be positive. If someone tries to sample from the Laplace distribution with a bad parameter value, they should get an error message telling them that the inputs are wrong.

In R, we can include error messages inside functions using the `stop()` function. Here is an example in which we write our own square root function:

```{r, error=T}
my_sqrt <- function(x){
  if(x < 0){
    stop("The square root of a negative number is complex")
  }
  
  x^(1/2)
}

my_sqrt(4)
my_sqrt(-4)
```


:::{.question}
#### Question 3 (code)

Write an R function called `my_rlaplace` to sample from the Laplace distribution using the inverse transform method, with the following requirements:

Inputs:

* `n`: the number of samples to take
* `mu`: the parameter $\mu$ of the Laplace defined above
* `b`: the parameter $b$ of the Laplace defined above; $b > 0$

Output: a vector of length $n$, containing the Laplace samples

Error handling: the function should throw an error and provide an informative error message if `b` is not strictly positive

Additional requirements: 

* You may not use any existing Laplace functions to write your code
* You must use the inverse transform method
* Use the `runif` function to generate your uniform sample

**Test cases:** See the unit tests in `hw3_tests.R`. The unit tests will test your error handling, sampling, and will perform a KS test to compare the generated sample with the Laplace distribution.

:::


## The Box-Muller transform

The inverse transform method is one way to generate samples from a particular distribution; however, the inverse transform method can be difficult to implement when there is no closed form solution for the inverse cdf. An important example of this scenario is the Normal distribution, which does not even have a closed form solution for the cdf, never mind the inverse cdf.

Fortunately, other methods can be used to generate random variables. In some cases, other transformations will produce the desired distribution; for example, a chi-square distribution can be created by summing independent squared standard normals.

In the case of the Normal distribution, one method is the **Box-Muller** transform.

**Definition (Box-Muller transform):** Let $U_1, U_2$ be independent $Uniform(0, 1)$ random variables. Define

$$X_1 = \sqrt{-2 \log(U_1)} \cos (2 \pi U_2)$$

$$X_2 = \sqrt{-2 \log(U_1)} \sin (2 \pi U_2)$$

Then, $X_1, X_2$ are independent $N(0, 1)$ random variables.

The goal of this portion of the assignment is to show that the Box-Muller transform works. To do so, we will need to revisit transformations of multiple random variables.

### Step 1: The goal

Recall that two variables $X_1, X_2$ are *independent* if their joint pdf factors as the product of the marginal pdfs:

$$f_{X_1, X_2}(x_1, x_2) = f_{X_1}(x_1) f_{X_2}(x_2)$$

To prove that $X_1,X_2$ are independent $N(0, 1)$ random variables therefore involves finding their joint pdf $f_{X_1, X_2}(x_1, x_2)$.

### Step 2: Re-writing the transformation

In the definition of the Box-Muller transformation above, we can see that both $X_1$ and $X_2$ are functions of $U_1$ and $U_2$; let's call these functions $g_1$ and $g_2$:

$$X_1 = g_1(U_1, U_2) = \sqrt{-2 \log(U_1)} \cos (2 \pi U_2)$$

$$X_2 = g_2(U_1, U_2) = \sqrt{-2 \log(U_1)} \sin (2 \pi U_2)$$

Now, in this case we can *invert* this process, to write $U_1$ and $U_2$ as functions of $X_1$ and $X_2$:

$$U_1 = h_1(X_1, X_2) = \exp \left\lbrace - \frac{(X_1^2 + X_2^2)}{2} \right\rbrace$$

$$U_2 = h_2(X_1, X_2) = \frac{1}{2\pi} \arctan \left( \frac{X_2}{X_1} \right)$$


:::{.question}
#### Question 4(a) (written)

Show that if $X_1 = \sqrt{-2 \log(U_1)} \cos (2 \pi U_2)$ and $X_2 = \sqrt{-2 \log(U_1)} \sin (2 \pi U_2)$, then $U_1 = \exp \left\lbrace - \frac{(X_1^2 + X_2^2)}{2} \right\rbrace$ and $U_2 = \frac{1}{2\pi} \arctan \left( \frac{X_2}{X_1} \right)$.

You may use fundamental trig identities. Show all work.
:::

### Step 3: transformations of multiple random variables

We want to find the joint density $f_{X_1, X_2}(x_1, x_2)$, and we have $U_1 = h_1(X_1, X_2)$ and $U_2 = h_2(X_1, X_2)$. We also know the joint pdf $f_{U_1, U_2}(u_1, u_2)$ (*hint:* $U_1$ and $U_2$ are independent $Uniform(0, 1)$!). How do we use this information to get $f_{X_1, X_2}$?

It turns out (you do not need to prove this) that

$$f_{X_1, X_2}(x_1, x_2) = f_{U_1, U_2}(h_1(x_1, x_2), h_2(x_1, x_2)) |\det({\bf J})|,$$

where $|\det ({\bf J})|$ is the absolute value of the determinant of the matrix of partial derivatives:


$$\det({\bf J}) = \begin{vmatrix} \frac{\partial u_1}{\partial x_1} & \frac{\partial u_1}{\partial x_2} \\ \frac{\partial u_2}{\partial x_1} & \frac{\partial u_2}{\partial x_2} \end{vmatrix}$$

and the partial derivatives are $\dfrac{\partial u_i}{\partial x_j} = \dfrac{\partial h_i(x_1, x_2)}{\partial x_j}$

This might look unfamiliar, but that's ok! We will break it down step-by-step.

### Partial derivatives

Suppose you have functions of multiple variables. For example, suppose we have

$$h_1(x_1, x_2) = 2x_1^2 + 3x_1x_2 + x_2$$

We'd like to understand how this function changes as we change $x_1$ (keeping $x_2$ fixed). To do this, just like in single variable calculus, we take the derivative with respect to $x_1$! Similarly, if we want to understand how the function changes as we change $x_2$, then we need to take a derivative with respect to $x_2$. Because there are other variables in the function, these derivatives are called *partial* derivatives.

To calculate a partial derivative, I just treat any other variables in the function as fixed when I calculate the derivative. For example,

$$\frac{\partial h_1(x_1, x_2)}{\partial x_1} = 4x_1 + 3x_2 \hspace{0.5cm} \text{(treating } x_2 \text{ as a constant here!)}$$

$$\frac{\partial h_1(x_1, x_2)}{\partial x_2} = 3x_1 +1 \hspace{0.5cm} \text{(treating } x_1 \text{ as a constant here!)}$$

:::{.question}
#### Question 4(b) (written)

Calculate the four partial derivatives we need:

* $\frac{\partial h_1(x_1, x_2)}{\partial x_1}$
* $\frac{\partial h_1(x_1, x_2)}{\partial x_2}$
* $\frac{\partial h_2(x_1, x_2)}{\partial x_1}$
* $\frac{\partial h_2(x_1, x_2)}{\partial x_2}$

Show all work.

*Hint:* Recall that the derivative of arctan is

$$\frac{d}{dx} \text{arctan} \ x = \frac{1}{1 + x^2}$$
:::

### The determinant

Recall that the determinant of a 2x2 matrix is 

$$\begin{vmatrix} a & b \\ c & d \end{vmatrix} = ad - bc$$

:::{.question}
#### Question 4(c) (written)

Using your answer to 4(b), calculate $|\det ({\bf J})|$, the absolute value of the determinant $\det ({\bf J})$. Note: your answer will be in terms of $x_1$ and $x_2$.

Show all work.
:::


### Putting it all together

:::{.question}
#### Question 4(d) (written)

Suppose that $X_1, X_2$ are generated by the Box-Muller transform, as defined above. Using your answers to the previous questions, prove that $X_1, X_2$ are independent, $N(0, 1)$ random variables.

Show all work.
:::


## Acceptance-rejection sampling

Another method for sampling from a distribution of interest is acceptance-rejection sampling. Suppose we wish to sample a random variable with probability function $f$, and we have the ability to sample from some other distribution with probability function $g$, such that for some $c > 0$,

$$\frac{f(t)}{g(t)} \leq c \hspace{1cm} \text{for all } t \text{ where } f(t) > 0$$


Acceptance-rejection sampling then generates $X \sim f$ as follows:

1. Sample $Y \sim g$
2. Sample $U \sim Uniform(0, 1)$
3. If $U \leq \dfrac{f(Y)}{cg(Y)}$, set $X = Y$. Otherwise, return to step 1.

### Rayleigh distribution

The $Rayleigh(\sigma)$ distribution has density
$$f(x) = \frac{x}{\sigma^2} e^{-\frac{x^2}{2 \sigma^2}}, \hspace{1cm} \text{ for } x \geq 0,$$
where $\sigma>0$ is the scale parameter of the distribution.

To do acceptance-rejection sampling, we need to choose a candidate distribution $g$. How should we do that?

* The Rayleigh distribution is supported on $[0, \infty)$, so we need $g$ to be supported on $[0, \infty)$ as well
* A distribution $g$ with a similar form ($x^{\text{something}} \cdot e^{x \cdot \text{something}})$) seems like a good choice

In this question, you will use a $Gamma(\alpha, \beta)$ distribution. Recall that this distribution has pdf

$$g(x) = \frac{1}{\Gamma(\alpha) \beta^\alpha} x^{\alpha - 1} e^{-x/\beta}$$

We then need to find the value 

$$c = \max_t \frac{f(t)}{g(t)}$$

:::{.question}
#### Question 5 (written)

To generate from a $Rayleigh(\sigma)$ distribution, suppose we choose $g$ to be the pdf of a $Gamma(2, 2\sigma^2)$. Using calculus, find the bound $c$.
:::

:::{.question}
#### Question 6 (code)

Using your answer to question 5, write a function called `my_rrayleigh` to simulate from the Rayleigh distribution with acceptance-rejection sampling, with the candidate density specified in question 5. Your function should satisfy the following requirements:

Inputs:

* `n`: the number of samples to take
* `scale`: the scale parameter $\sigma$ of the Rayleigh distribution; $\sigma > 0$

Output: a vector of length $n$, containing the Rayleigh samples

Error handling: the function should throw an error and provide an informative error message if $\sigma \leq 0$

Additional requirements: 

* You may not use any existing Rayleigh functions to write your code
* You must use acceptance-rejection sampling, which you implement yourself
* You *may* (and should) use existing R functions to simulate from your candidate distribution $g$
* Use the `runif` function to generate uniform samples

**Test cases:** See the unit tests in `hw3_tests.R`. The unit tests will test your error handling and sampling.
:::
