---
title: "Homework 3"
output: 
  rmdformats::robobook:
    css: "homework.css"
    highlight: pygments
link-citations: yes
---

**Due:** Friday February 7, 10:00am

**Submission:** For this assignment, you will submit two files:

* An R script (`hw3_code.R`) containing your functions for questions 1, 3, and 6
* A pdf containing your written answers to questions 2, 4, and 5

**Mastery:** To master this assignment, you must satisfy the following requirements:

* Your code must pass all specified unit tests, *as written* (you may not modify the unit tests which I provide)
* Your code must meet all requirements described in the questions below (matching the requested inputs and outputs)
* You must comment your code
* Your written solutions must be complete and correct, with no missing steps or mistakes
* Your work must abide by the academic honesty requirements in the syllabus 

If you make an honest effort to answer all questions, but you do not master all of the questions on your first submission, you will have one resubmission attempt after receiving feedback.

**Resources:** 

* *Statistical Computing with R* (Risso), Chapter 3
* *Computational Statistics* (Givens and Hoeting), Chapter 6.2

Both books are available electronically through the ZSR library

## Preliminaries: assessing generated random variables

In class, we have talked about several different methods for generating random variables. A crucial step when simulating a random variable is checking that it does actually follow the desired distribution!

One way we did this in class was to use the empirical cdf, and compare the empirical cdf to the true cdf (which should be known if we are trying to simulate from a specific distribution). While visually comparing cdfs is a valuable approach, we might wonder whether we can formalize this comparison. That is, can we conduct a hypothesis test that would detect significant differences between the empirical cdf and the true cdf?

The answer is yes, and comes in the form of the **Kolmogorov-Smirnov** (KS) test. In the first part of this assignment you will implement the KS test, to get a better feel for what the test is doing. In the remainder of the assignment, the KS test will be used in our unit tests to help assess the random variables we generate.

### The Kolmogorov-Smirnov test

Suppose we observe a univariate random sample $X_1,...,X_n$ of size $n$, from some population. We are interested in testing whether the generating distribution for this sample is some specified distribution; for example, we may wish to know whether the data come from a $Uniform(0, 1)$ distribution, or a $N(0, 1)$ distribution.

Formally, let $F$ be the (unknown) distribution function (cdf) for the sample, i.e. $X_1,...,X_n \overset{iid}{\sim} F$. Let $F_0$ be the *hypothesized* distribution function (e.g., a particular uniform, or standard normal, or something else). We wish to test the following hypotheses:

$$H_0: F = F_0$$
$$H_A: F \neq F_0$$

Note that this is a very strong null hypothesis. Often when testing hypotheses, we ask something about a single parameter, like the mean. Here, however, we are asking whether we know the entire *distribution*. In certain cases, however, this is the right kind of null hypothesis; when generating random numbers, for example, we really do want them to be $Uniform(0, 1)$.

How does one test such hypotheses? One option, called the *Kolmogorov-Smirnov* (KS) test, utilizes our old friend the empirical cdf. In particular, the KS test is built by examining the maximal distance between the empirical cdf $\widehat{F}$, and the hypothesized cdf $F_0$:

$$D = \sup \limits_t |\widehat{F}(t) - F_0(t)|$$

Now, you may be wondering, how does one compute a supremum like this, if it is over uncountably many points? Fortunately, properties of the cdf mean that we really only need to consider the $n$ observed points. 

Let $X_{(1)},...,X_{(n)}$ denote the *ordered* sample (from smallest to largest). Then, it turns out that 

$$D = \max \limits_{1 \leq i \leq n} \left \lbrace F_0(X_{(i)}) - \frac{i-1}{n}, \frac{i}{n} - F_0(X_{(i)})\right\rbrace$$

We then reject the null hypothesis that $F = F_0$ when our empirical cdf $\widehat{F}$ (i.e., our best estimate of the true cdf $F$) is far from the hypothesized cdf $F_0$. That is, we reject when $D$ is large!

Here is an example of the KS test in action, using the `ks.test` function in R. First, we generate 100 samples from a $Uniform(0, 1)$ distribution, then test if they come from a $Uniform(0, 1)$:

```{r}
set.seed(27)
x <- runif(100)
ks.test(x, punif)
```
Here the test statistic is $D = 0.084932$, and the p-value is 0.4664. That's quite a high p-value, so we fail to reject. Since the data really did come from a $Uniform(0, 1)$ distribution, that's good news!

On the other hand, if we sample from a standard normal, but hypothesize that our data comes from a uniform distribution, we would hope to *reject* the null:

```{r}
set.seed(27)
x <- rnorm(100)
ks.test(x, punif)
```

### Computing the p-value

The `ks.test` function in R provides a p-value, but it is not immediately clear where that p-value comes from. What distribution should this maximal difference $D$ follow?

The math is a bit beyond the scope of this course, but it turns out that asymptotically (that is, for a sufficiently large sample size),

$$P(\sqrt{n} D \leq t) \approx \frac{\sqrt{2 \pi}}{t} \sum \limits_{k=1}^\infty e^{-(2k - 1)^2 \pi^2 / (8t^2)}$$

Now, an infinite sum here seems tricky to calculate, but fortunately it converges quickly; you probably don't need more than about 30 terms to get a good approximation of the cdf, so 

$$P(\sqrt{n} D \leq t) \approx \frac{\sqrt{2 \pi}}{t} \sum \limits_{k=1}^{30} e^{-(2k - 1)^2 \pi^2 / (8t^2)}$$

### Summary

To summarize, the KS test works like this: given a sample $X_1,...,X_n$, the test statistic is

$$D = \max \limits_{1 \leq i \leq n} \left \lbrace F_0(X_{(i)}) - \frac{i-1}{n}, \frac{i}{n} - F_0(X_{(i)})\right\rbrace$$

and the approximate p-value is

$$\text{p-value} \approx 1 - \frac{\sqrt{2 \pi}}{\sqrt{n} D} \sum \limits_{k=1}^{30} e^{-(2k - 1)^2 \pi^2 / (8 n D^2)}$$

:::{.question}
#### Question 1

Write a function in R called `my_kstest`, which satisfies the following requirements:

Inputs:

* `x`: a vector, containing the random sample
* `cdf`: a function (such as `pnorm`, `punif`, etc.) specifying the distribution function $F_0$ under the null hypothesis

Output: a list, containing the following named elements:

* `D`: the test statistic $D$
* `pval`: the p-value for the test

Additional requirements:

* You may not use any existing KS test implementations to write your function
* You must implement the p-value approximation using the cdf defined above; you may not use any R packages that provide functions for the Kolmogorov distribution

**Test cases:** See the unit tests in `hw3_tests.R`

:::


## The inverse transform method

In previous activities and assignments, you have worked on generating (pseudo-random) $Uniform(0, 1)$ samples. One method of generating other types of random variables is to use these uniform samples as a building block, and applying transformations to produce the distribution we want. When tractable, an attractive approach is the *inverse transform method*, in which we apply the inverse cdf to a uniform sample.

**Definition:** (inverse transform method) Let $U \sim Uniform(0, 1)$ be a uniform random variable, and let $F$ be the cdf of a distribution we would like to sample from (either continuous or discrete). Define
$$F^{-1}(u) = \inf \{x: F(x) \geq u\}$$
and let $X = F^{-1}(U)$. Then, $P(X \leq t) = F(t)$, i.e. $X$ has the desired cdf and is therefore a sample from the desired distribution.

In the following questions, you will use the inverse transform method to simulate from a specified distribution. You will submit one written question in which you derive the inverse cdf, and one code-based question in which you use the inverse cdf method to sample from the desired distribution.


### Laplace distribution


The $Laplace(\mu, b)$ distribution has pdf

$$f(x) = \frac{1}{2b} \exp\left\lbrace - \frac{|x - \mu|}{b} \right\rbrace$$
where $x, \mu \in \mathbb{R}$ and $b > 0$.


:::{.question}
#### Question 2 (written)

Derive the inverse transform for the $Laplace(\mu, b)$ distribution. That is, if $U \sim Uniform(0, 1)$, find $F^{-1}(U)$ as a function of $U$. Show all work.

:::


Now let's implement a function to simulate from the Laplace distribution. Before we do, however, we should think about possible errors that could arise when calling our function. Note from the definition above that the parameter $b$ for the Laplace must both be positive. If someone tries to sample from the Laplace distribution with a bad parameter value, they should get an error message telling them that the inputs are wrong.

In R, we can include error messages inside functions using the `stop()` function. Here is an example in which we write our own square root function:

```{r, error=T}
my_sqrt <- function(x){
  if(x < 0){
    stop("The square root of a negative number is complex")
  }
  
  x^(1/2)
}

my_sqrt(4)
my_sqrt(-4)
```


:::{.question}
#### Question 3 (code)

Write an R function called `my_rlaplace` to sample from the Laplace distribution using the inverse transform method, with the following requirements:

Inputs:

* `n`: the number of samples to take
* `mu`: the parameter $\mu$ of the Laplace defined above
* `b`: the parameter $b$ of the Laplace defined above; $b > 0$

Output: a vector of length $n$, containing the Laplace samples

Error handling: the function should throw an error and provide an informative error message if `b` is not strictly positive

Additional requirements: 

* You may not use any existing Laplace functions to write your code
* You must use the inverse transform method
* Use the `runif` function to generate your uniform sample

**Test cases:** See the unit tests in `hw3_tests.R`. The unit tests will test your error handling, sampling, and will perform a KS test to compare the generated sample with the Laplace distribution.

:::


## The Box-Muller transform

The inverse transform method is one way to generate samples from a particular distribution; however, the inverse transform method can be difficult to implement when there is no closed form solution for the inverse cdf. An important example of this scenario is the Normal distribution, which does not even have a closed form solution for the cdf, never mind the inverse cdf.

Fortunately, other methods can be used to generate random variables. In some cases, other transformations will produce the desired distribution; for example, a chi-square distribution can be created by summing independent squared standard normals.

In the case of the Normal distribution, one method is the **Box-Muller** transform.

**Definition (Box-Muller transform):** Let $U_1, U_2$ be independent $Uniform(0, 1)$ random variables. Define

$$X_1 = \sqrt{-2 \log(U_1)} \cos (2 \pi U_2)$$

$$X_2 = \sqrt{-2 \log(U_1)} \sin (2 \pi U_2)$$

Then, $X_1, X_2$ are independent $N(0, 1)$ random variables.

The goal of this portion of the assignment is to show that the Box-Muller transform works. To do so, we will need to revisit transformations of multiple random variables.

### Transformations of multiple random variables

Let $(X_1,...,X_n)'$ be a continuous random vector with joint pdf $f_X(x_1,...,x_n)$. Let $(Y_1,...,Y_n)'$ be a continuous random vector defined by

$$Y_1 = g_1(X_1,...,X_n) \hspace{1cm} Y_2 = g_2(X_1,...,X_n) \hspace{1cm} \text{etc.}$$
Assume that the functions $g_1,...,g_n$ are such that for all $(y_1,...,y_n)$ there is exactly one $(x_1,...,x_n)$ such that $(y_1,...,y_n) = (g_1(x_1,...,x_n),...,g_n(x_1,...,x_n))$. That is, collectively the transformation is invertible, and we can define functions $h_1,...,h_n$ such that $x_1 = h_1(y_1,...,y_n)$, $x_2 = h_2(y_1,...,y_n)$, etc.

Then, the joint pdf of $(Y_1,...,Y_n)'$ is given by

$$f_Y(y_1,...,y_n) = f_X(h_1(y_1,...,y_n), ..., h_n(y_1,...,y_n)) \ |\det({\bf J})|$$

where $|\det ({\bf J})|$ is the absolute value of the determinant of the matrix of partial derivatives:

$$\det({\bf J}) = \begin{vmatrix} \frac{\partial x_1}{\partial y_1} & \frac{\partial x_1}{\partial y_2} & \cdots & \frac{\partial x_1}{\partial y_n} \\ \frac{\partial x_2}{\partial y_1} & \frac{\partial x_2}{\partial y_2} & \cdots & \frac{\partial x_2}{\partial y_n} \\ \vdots & \vdots & \cdots & \vdots \\ \frac{\partial x_n}{\partial y_1} & \frac{\partial x_n}{\partial y_2} & \cdots & \frac{\partial x_n}{\partial y_n} \end{vmatrix}$$

and the partial derivatives are $\dfrac{\partial x_i}{\partial y_j} = \dfrac{\partial h_i(y_1,...,y_n)}{\partial y_j}$

### Sidebar: partial derivatives

Suppose you have a function of multiple variables. For example, consider

$$f(x, y) = ax^2 + bxy + cy^2$$

We'd like to understand how that function changes as we change $x$. To do this, we can take a derivative with respect to $x$. Similarly, if we want to understand how the function changes as we change $y$, then we need to take a derivative with respect to $y$. Because there are other variables in the function, these derivatives are called *partial* derivatives.

To calculate a partial derivative, I just treat any other variables in the function as fixed when I calculate the derivative. For example,

$$\frac{\partial f}{\partial x} = 2ax + by \hspace{0.5cm} \text{(treating y as a constant here!)}$$


:::{.question}
#### Question 4 (written)

Suppose that $X_1, X_2$ are generated by the Box-Muller transform, as defined above. Prove that $X_1, X_2$ are independent, $N(0, 1)$ random variables.

*Hints:*

* Begin by inverting the transform to find $U_1$ and $U_2$ as functions of $X_1$ and $X_2$
* You may wish to revisit facts about trigonometric functions, such as their derivatives. You may look these facts up as needed

Show all work.
:::


## Acceptance-rejection sampling

Another method for sampling from a distribution of interest is acceptance-rejection sampling. Suppose we wish to sample a random variable with probability function $f$, and we have the ability to sample from some other distribution with probability function $g$, such that for some $c > 0$,

$$\frac{f(t)}{g(t)} \leq c \hspace{1cm} \text{for all } t \text{ where } f(t) > 0$$


Acceptance-rejection sampling then generates a sample $x$ from $f$ as follows:

1. Sample $y \sim g$
2. Sample $u \sim Uniform(0, 1)$
3. If $\dfrac{f(y)}{cg(y)} \leq u$, set $x = y$. Otherwise, return to step 1.

### Rayleigh distribution

The $Rayleigh(\sigma)$ distribution has density
$$f(x) = \frac{x}{\sigma^2} e^{-\frac{x^2}{2 \sigma^2}}, \hspace{1cm} \text{ for } x \geq 0,$$
where $\sigma>0$ is the scale parameter of the distribution.

:::{.question}
#### Question 5 (written)

Propose a candidate distribution $g$ for acceptance-rejection sampling from the Rayleigh distribution. Explain your choice of $g$, and find the value $c$.

*Hint:* The Rayleigh distribution is supported on $[0, \infty)$. Think about other continuous distributions supported on $[0, \infty)$ which also involve $x^{\text{something}}e^{-\text{something else} \cdot x}$
:::

:::{.question}
#### Question 6 (code)

Using your answer to question 5, write a function called `my_rrayleigh` to simulate from the Rayleigh distribution with acceptance-rejection sampling. Your function should satisfy the following requirements:

Inputs:

* `n`: the number of samples to take
* `scale`: the scale parameter $\sigma$ of the Rayleigh distribution; $\sigma > 0$

Output: a vector of length $n$, containing the Rayleigh samples

Error handling: the function should throw an error and provide an informative error message if $\sigma \leq 0$

Additional requirements: 

* You may not use any existing Rayleigh functions to write your code
* You must use acceptance-rejection sampling
* You *may* (and should) use existing R functions to simulate from your candidate distribution $g$
* Use the `runif` function to generate uniform samples

**Test cases:** See the unit tests in `hw3_tests.R`. The unit tests will test your error handling and sampling.
:::
