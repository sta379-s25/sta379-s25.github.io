---
title: "Project 3: The BFGS algorithm"
output: 
  rmdformats::robobook:
    css: "homework.css"
    highlight: pygments
link-citations: yes
---

**Due:** 

* If you would like the option to resubmit the project after receiving feedback and a grade on your initial submission, the due date is **Monday, April 28, 10:00pm** (you may, of course, use up to 2 extension days from your bank)

* If you waive the option to resubmit the project, the due date is **Wednesday, May 7 at 2pm**. Important: this is the last day of final exams, and **no extension days** may be used to submit the project beyond this date.

**Submission:** See the Submission Checklist at the end of the assignment

# Rules

Projects in this course take the place of exams. Their goal is both to assess what you have learned in the class, and to explore extensions of the material more complex than we can cover on a homework assignment. Therefore, projects **must be completed independently**. You **may not:**

* Discuss the project with anyone other than the course instructor
* Share tips, hints, suggestions, code snippets, full or partial solutions, etc. to any of the pieces of this project with anyone other than the course instructor
* Use any other resource (a book, the internet, generative AI such as ChatGPT, etc.) to find solutions to mathematical questions or the coding implementation
* Read any other implementations or pseudocode for the coding portion of the project

However, you **may:**

* Discuss the project with me in office hours (however, I may provide somewhat less specific assistance than on homework assignments)
* Refer to any of your previous assignments and feedback
* Refer to other resources (the internet, books, notes from prior courses) for short questions not specific to the project assignment. Such questions could include things like
    * How do I call an R function in C++ with Rcpp?
    * How do I find the length of a vector in C++?
    * What does [a specific error message] mean?
    
Any violation of the rules will be treated as plagiarism, and may result in a 0 on the assignment with no option of resubmission.


# Background: Newton's method and quasi-Newton methods

Suppose we wish to optimize some function $f: \mathbb{R}^n \to \mathbb{R}$. Recall that Newton's method uses an iterative update rule:

$${\bf x}^{(k+1)} = {\bf x}^{(k)} - \alpha_k({\bf H}_f({\bf x}^{(k)}))^{-1} \nabla f({\bf x}^{(k)})$$

where we begin with an initial guess ${\bf x}^{(0)}$, $\nabla f$ is the gradient, and ${\bf H}_f$ is the hessian. The "natural" value of $\alpha_k$ is $\alpha_k = 1$, but sometimes it is necessary to choose a different value of $\alpha_k$, in which case a procedure such as backtracking line search may be used.

However, a challenge of Newton's method is that it involves inverting the hessian ${\bf H}_f$. For higher-dimensional problems (i.e., $n$ large), computing the inverse becomes an expensive operation and we may wish to avoid it. Instead, suppose we *approximate* the inverse hessian ${\bf H}_f^{-1}$ with some matrix ${\bf B}^{(k)}$. Then the update rule would look like

$${\bf x}^{(k+1)} = {\bf x}^{(k)} - \alpha_k {\bf B}_f({\bf x}^{(k)}) \nabla f({\bf x}^{(k)})$$

This is called a *quasi-Newton* method.

## The BFGS algorithm

One common approach to approximating the inverse hessian is the Broyden-Fletcher-Goldfarb-Shanno (BFGS) algorithm.

Given a current guess ${\bf x}^{(k)}$ and a current inverse hessian approximation ${\bf B}^{(k)}$, the BFGS algorithm updates ${\bf x}^{(k)}$ by:

* $d_k = - {\bf B}^{(k)} \nabla f({\bf x}^{(k)})$
* $s_k = \alpha_k d_k$, where $\alpha_k$ is chosen to satisfy the **Wolfe conditions**
* ${\bf x}^{(k + 1)} = {\bf x}^{(k)} + s_k$

We then update the inverse hessian approximation ${\bf B}^{(k)}$ by:

* $y_k = \nabla f({\bf x}^{(k+1)}) - \nabla f({\bf x}^{(k)})$
* $\rho_k = \dfrac{1}{y_k^T s_k}$
* ${\bf B}^{(k+1)} = ({\bf I} - \rho_k s_k y_k^T) {\bf B}^{(k)}({\bf I} - \rho_k y_k s_k^T) + \rho_k s_k s_k^T$ (where ${\bf I}$ is the identity matrix)

### Initialization

The user provides an initial choice ${\bf x}^{(0)}$. One possible choice for ${\bf B}^{(0)}$ is ${\bf B}^{(0)} = {\bf I}$.

### Convergence

Iterate until

$$||\nabla f({\bf x}^{(k)})|| < \varepsilon$$
for some small number $\varepsilon$, or until a maximum number of iterations have been reached.

### Wolfe conditions

The step size $\alpha_k$ in the BFGS algorithm should satisfy the following two conditions:

1. $f({\bf x}^{(k)} + \alpha_k d_k) \leq f({\bf x}^{(k)}) + c_1 \alpha_k d_k^T \nabla f({\bf x}^{(k)})$
2. $d_k^T \nabla f({\bf x}^{(k)} + \alpha_k d_k) \geq c_2  d_k^T \nabla f({\bf x}^{(k)})$

for values $0 < c_1 < c_2 < 1$. Common choices are $c_1 = 10^{-4}$ and $c_2 = 0.9$. 

Such a step size $\alpha_k$ can be found via a line search.

## Example in R

Consider a logistic regression model:

$$Y_i \sim Bernoulli(p_i)$$
$$\log \left( \frac{p_i}{1 - p_i} \right) = \beta^T X_i$$

The code below generates data from a logistic regression model, and returns the fitted coefficients from the `glm` function in R:

```{r}
set.seed(123)
n <- 2000
d <- 5 # number of explanatory variables
X <- cbind(1, matrix(rnorm(n*d), ncol = d))
beta <- runif(d + 1, 0, 0.3)
p <- c(exp(X %*% beta)/(1 + exp(X %*% beta)))
y <- rbinom(n, 1, p)

glm.fit(X, y, family=binomial())$coefficients
```

The `glm` function in R fits models like logistic regression with Fisher scoring, which is essentially Newton's method. This, of course, involves solving systems with matrices; in particular, we may need to compute $({\bf X}^T {\bf W} {\bf X})^{-1}$, where ${\bf X}$ is the design matrix and ${\bf W} = \text{diag}(p_i(1-p_i))$ for logistic regression.

What if we want to use the BFGS algorithm instead? In R, we could use the `optim` function, which includes support for a variety of optimization algorithms, including BFGS (the Nelder-Mead algorithm, which we also discussed earlier in the semester, is another aglorithm supported by `optim`).

To use BFGS with the `optim` function, we need to provide the starting point ${\bf x}^{(0)}$, the function $f$, and the gradient function $\nabla f$:

```{r}
fn <- function(X, y, beta){
  mu <- c(exp(X %*% beta)/(1 + exp(X %*% beta)))
  -1*sum(dbinom(y, 1, mu, log=T))
}

gr <- function(X, y, beta){
  mu <- c(exp(X %*% beta)/(1 + exp(X %*% beta)))
  return(-1*t(X) %*% (y - mu))
}

beta_init <- c(log(mean(y)/(1 - mean(y))), rep(0, ncol(X) - 1))

optim(beta_init, function(beta){fn(X, y, beta)}, 
                   function(beta){gr(X, y, beta)}, method="BFGS",
                   control=list(trace=1))$par
```
We can see that the estimates are essentially the same as those returned by the `glm` function in R!

# Simulation: understanding the impact of data size

The BFGS algorithm has benefits when working with large numbers of explanatory variables in our models. For very large numbers of explanatory variables, we may also prefer to use another BFGS variant, called the limited-memory BFGS algorithm (L-BFGS). The L-BFGS algorithm is also available in the `optim` function.

In the simulated data above, the size of the data is controlled by the number of samples `n`, and the number of explanatory variables `d`.

:::{.question}
#### Question 1 (html)

Using the code above as a starting point, conduct a series of simulations to study how the size of our data (`n` and `d`) impact the time it takes to fit the logistic regression model. Compare the run times of Fisher scoring (with `glm.fit`), the BFGS algorithm (`optim` with `method = "BFGS"`), and the L-BFGS algorithm (`optim` with `method = "L-BFGS-B"`), for different combinations of `n` and `d`. 

Tools like the `bench::mark` function may be useful for timing the different methods.

The result of your simulations should be a plot, showing the different run times for each method as we change `n` and `d`. Discuss the trends you observe.
:::


# Positive definite matrices

An important piece of the BFGS algorithm is that the inverse Hessian approximation ${\bf B}^{(k)}$ is a **positive definite matrix**. 

**Definition:** Let $A \in \mathbb{R}^{m \times m}$ be a square, symmetric matrix with real entries. We say that $A$ is *positive definite* (PD) if for any vector $x \in \mathbb{R}^m$, $x \neq 0$, we have
$$x^T A x > 0.$$

## Practice with positive definite matrices

:::{.question}
#### Question 2 (written)

Let $X \in \mathbb{R}^{n \times m}$ have full column rank, and let $W \in \mathbb{R}^{n \times n}$ be a diagonal matrix with strictly positive entries on the main diagonal. Show that $X^T W X$ is positive definite.
:::

:::{.question}
#### Question 3 (written)

Let $A, B \in \mathbb{R}^{m \times m}$, and suppose both $A$ and $B$ are positive definite. Prove that $A + B$ is also positive definite.
:::

:::{.question}
#### Question 4 (written)

 Let $A = \begin{bmatrix} 1 & a \\ a & b \end{bmatrix}$. If $a^2 < b$, show that $A$ is positive definite.
:::


:::{.question}
#### Question 5 (written)

Consider square matrices $A \in \mathbb{R}^{m \times m}$, where $A_{ii} = 2$, $A_{i,i+1} = A_{i+1,i} = -1$, and all other entries are 0. For example, with $m = 4$, the matrix looks like
$$A = \begin{bmatrix} 2 & -1 & 0 & 0 \\ -1 & 2 & -1 & 0 \\ 0 & -1 & 2 & -1 \\ 0 & 0 & -1 & 2 \end{bmatrix}$$
Prove that any such matrix is positive definite.
:::

:::{.question}
#### Question 6 (html)

For the matrix $A$ is question 5, consider the function $f:\mathbb{R}^m \to \mathbb{R}$ defined by

$$f(x) = x^T A x + 7$$

Show that $f$ has a unique minimum at $x = 0$, then use your implementation of the BFGS algorithm to find the minimum when $m = 4$ (the result from BFGS should be very close to 0, though numerically it won't be exactly equal to 0).
:::

:::{.question}
#### Question 7 (written)

The update rule for the inverse Hessian approximation ${\bf B}^{(k)}$ in the BFGS algorithm is

$${\bf B}^{(k+1)} = ({\bf I} - \rho_k s_k y_k^T) {\bf B}^{(k)}({\bf I} - \rho_k y_k s_k^T) + \rho_k s_k s_k^T$$

Show that if ${\bf B}^{(k)}$ is positive definite and $\rho_k > 0$, then ${\bf B}^{(k+1)}$ is positive definite.
:::

# Implementation: Your task

Your task in this assignment is to write your own implementation of the BFGS algorithm, with C++ and Rcpp / RcppArmadillo. Your implementation should satisfy the following requirements. *Note*: it is very important that you name your arguments and variables just as I request here, so that I can test whether your code runs correctly.

* The main function should be called `bfgs`, with the following requirements:
    * Inputs:
        * `xk`: initial parameter estimates (similar to `par` in the `optim` function)
        * `fn`: an `Rcpp::Function` (a function written in R, which can be passed to `bfgs`), which is the function $f$ to optimize
        * `gr`: an `Rcpp::Function`, which is the gradient function $\nabla f$
        * `max_iter`: an integer specifying the maximum number of iterations for the BFGS algorithm. The default should be 1000.
        * `tol`: the tolerance ($\varepsilon$) for determining convergence of the algorithm. Default should be $1 \times 10^{-6}$
    * Output: a vector containing the final parameter estimates (i.e., the final ${\bf x}^{(k)}$ when iteration stops)
* You should write helper functions that are called by `bfgs`
* All code (`bfgs` and any helper functions) should be written in C++ using the Rcpp and/or RcppArmadillo libraries (you will almost certainly need to use Armadillo, for the linear algebra required)
* All code must be documented
* Your C++ code should be written in `project3_code.cpp`
* The `bfgs` function must be callable from R, after sourcing `project3_code.cpp`

## Unit tests

**You** are responsible for creating `testthat` unit tests for all functions in this project. These unit tests should check things like:

* Do we get the correct type of output?
* Do we get the right size of output? 
* Do we get the right values of output?

I recommend writing several functions to optimize (the logistic regression code above gives one such function, but you should also pick some others), and checking that the results from your `bfgs` function are close to the results from R's `optim` function.


# Grading rubric

Each component of the assignment will be graded as mastered / not yet mastered. If you make an honest attempt at a component but do not master that component on your initial submission, you might have one attempt to re-submit and master that component (depending on when you initially submit your work). Note that mastering the `bfgs` component requires that the function be fully correct and complete; if it does not pass one or more of the unit tests, there is no partial credit for the `bfgs` component.

| Component | Portion of total grade | Minimum requirements to receive credit |
| --- | --- | --- |
| Main `bfgs` function | 40% | Function meets requirements listed above and passes all provided unit tests |
| Use of helper functions | 5% | Computation is reasonably divided with helper functions, each with a clear task |
| Code style and readability | 5% | Code is commented, clearly written, and uses informative names |
| Unit tests | 15% | Several unit tests are written in `project3_tests.R` for the main `bfgs` function and for any helper functions, testing that all functions work as desired. Functions pass the provided unit tests |
| Simulation and mathematical questions | 35% (5% each) | Each question will be graded for mastery. Mastery of mathematical questions requires a fully complete and correct solution, with no errors or missing steps/explanation. Mastery of simulation questions requires full code, results and discussion. All necessary figures should be included, and should be properly labeled, clear, and easy to read. Discussion should explain the results shown in the figures in complete sentences. The simulation and mathematical questions will be treated separately, e.g. mastering 6 of the 7 questions would give 30 out of 35 possible points |


# Submission checklist

* pdf with written solutions to questions 2,3,4,5,7 (Canvas)
* HTML with simulation code, results, plots, and discussion for questions 1 and 6 (Canvas)
* implementation in `project3_code.cpp` (GitHub)
    * main `bfgs` function, following all listed requirements
    * any associated helper functions
* unit tests in `project3_tests.R` (GitHub)
    * You are responsible for creating unit tests for **all** functions which you write. I suggest you use the `optim` function to help check your answers (you will not get exactly the same results, but they should be pretty close)

